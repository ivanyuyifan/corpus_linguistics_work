Web Science: Studying the Internet to Protect Our Future
Studying the Web will reveal better ways to exploit information, prevent identity theft, revolutionize industry and manage our ever growing online lives 
Since the World Wide Web blossomed in the mid-1990s, it has exploded to more than 15 billion pages that touch almost all aspects of modern life. 
Today more and more people's jobs depend on the Web. 
Media, banking and health care are being revolutionized by it. 
And governments are even considering how to run their countries with it. 
Little appreciated, however, is the fact that the Web is more than the sum of its pages. 
Vast emergent properties have arisen that are transforming society. 
E-mail led to instant messaging, which has led to social networks such as Facebook. 
The transfer of documents led to file-sharing sites such as Napster, which have led to user-generated portals[1] such as YouTube. 
And tagging content with labels is creating online communities that share everything from concert news to parenting tips.
But few investigators are studying how such emergent properties have actually happened, how we might harness them, what new phenomena may be coming or what any of this might mean for humankind. 
A new branch of science§Ó "Web science§Ó" aims to address such issues. 
The timing[2] fits history: computers were built first, and computer science followed, which subsequently improved computing significantly. 
Web science was launched as a formal discipline in November 2006, when the two of us and our colleagues at the Massachusetts Institute of Technology and the University of Southampton in England announced the beginning of a Web Science Research Initiative. 
Leading researchers from 16 of the world§Ó??s top universities have since expanded on that effort.
This new discipline will model the Web§Ó??s structure, articulate the architectural principles that have fueled its phenomenal[3] growth, and discover how online human interactions are driven by and can change social conventions. 
It will elucidate the principles that can ensure that the network continues to grow productively and settle complex issues such as privacy protection and intellectual-property rights. 
To achieve these ends, Web science will draw on mathematics, physics, computer science, psychology, ecology, sociology, law, political science, economics, and more.
Of course, we cannot predict what this nascent endeavor might reveal. 
Yet Web science has already generated crucial insights, some presented here. 
Ultimately, the pursuit[4] aims to answer fundamental questions: What evolutionary patterns have driven the Web's growth? Could they burn out? 
How do tipping points[5] arise, and can that be altered?
Insights Already 
Although Web science as a discipline is new, earlier research has revealed the potential value of such work. 
As the 1990s progressed, searching for information by looking for key words among the mounting number of pages was returning more and more irrelevant content. 
The founders of Google, Larry Page and Sergey Brin, realized they needed to prioritize[6] the results.
Their big insight was that the importance of a page "how relevant it is" was best understood in terms of the number and importance of the pages linking to it. 
The difficulty was that part of this definition is recursive: the importance of a page is determined by the importance of the pages linking to it, whose importance is determined by the importance of the pages linking to them. 
Page and Brin figured out an elegant[7] mathematical way to represent that property and developed an algorithm they called PageRank to exploit the recursiveness, thus returning pages ranked from most relevant to least.
Google's success shows that the Web needs to be understood and that it needs to be engineered[8]. 
Web science serves both purposes. 
The Web is an infrastructure of languages and protocols§Ó?¡±a piece of engineering. 
The philosophy[9] of content linking underlies[10] the emergent properties, however. 
Some of these properties are desirable and therefore should be engineered in. 
For example, ensuring that any page can link to any other page makes the Web powerful both locally and globally. 
Other properties are undesirable and if possible should be engineered out, such as the ability to build a site with thousands of artificial links generated by software robots for the sole intention of improving that site's search ranking--so-called link farms.
Another early discovery, which came from graph theory, is that the Web's connectivity follows a so-called power-law degree distribution. 
In many networks, nodes have about the same number of links to them. 
But on the Web a few pages have a huge number of other pages linking to them, and a very large number of pages have only a few pages linking to them. 
Northeastern University's Albert-L¨¢szl¨® Barab¨¢si and his colleagues coined the term "scale-free" to characterize such networks [see"Scale-Free Networks," by Albert-L¨¢szl¨® Barab¨¢si and Eric Bonabeau; Scientific American, May 2003]. 
Many people were surprised because they assumed Web pages would have an average number of links to and from them.
In scale-free networks, even if a majority of nodes are removed, a path from one of the remaining nodes to any other is still likely to exist. 
Removing a relatively small number of the highly connected nodes, or hubs, however, leads to significant disintegration. 
This analysis has been crucial for the companies and organizations§Ó?¡±be they telecommunications providers or research laboratories§Ó?¡±that design how information is routed on the Web, allowing them to build in substantial redundancy that balances traffic and makes the network more resistant to attack..
Thorough understanding of scale-free networks, gleaned by analyzing the Web, has prompted experts to analyze other network systems. 
They have since found power-law degree distributions in areas as far-flung[11] as scientific citations and business alliances. 
The work has helped the U.S. Centers for Disease Control and Prevention improve its models of sexual disease transmission and has helped biologists better understand protein interactions.
Scientific analysis has also characterized the Web as having short paths and small worlds. 
While at Cornell University in the 1990s, Duncan J. Watts and Steven H. Strogatz showed that even though the Web was huge, a user could get from one page to any other page in at most 14 clicks. 
Yet to fully understand these traits, we need to appreciate that the Web is a social network. 
In 1967 Harvard University psychologist Stanley Milgram asked residents in Omaha, Neb., and Wichita, Kan., to attempt to send a package to an individual described only by his name, some general features and the fact that he lived in Boston. 
The residents were to send the package to an intermediary who they thought might know more about how to reach the individual and who could then send it on to another intermediary. 
Eventually 64 of the almost 300 packages made it to the designated recipients. 
On average the number of intermediaries needed was six§Ó?¡±the basis of the catch-phrase "six degrees of separation."
More recently, however, Watts, now at Columbia University, tried to repeat the experiment with an e-mail message to be forwarded on the Web and experienced failures in path finding. 
In particular, if individuals had no incentive to forward the note the paths broke down. 
Yet only very slight incentives improved matters.
The lesson is that network structure alone is not everything; networks thrive only in the light of[12] the actions, strategies and perceptions of the individuals embedded in them. 
To realistically know why the Web has a beneficial structure of short paths, we need to know why people who contribute content link it to other material. 
Social drivers --goals, desires, interests and attitudes--are fundamental aspects of how links are made. 
Understanding the Web requires insights from sociology and psychology every bit as much as from mathematics and computer science.From Micro to Macro One major area of Web science will explore how a small technical innovation can launch[13] a large social phenomenon. 
A striking example is the emergence of the blogosphere [14]. 
Although early Web browsers did not provide a handy way for the average person to "publish" his or her ideas, by 1999 blog programs had made self-publishing much easier. 
Blogging subsequently caught fire because as people got issues off their chest, they also found others with similar views who could readily assemble into a like-minded community.
It is difficult to estimate the size of the blogosphere accurately. 
David Sifry's leading blog search engine, called Technorati, was tracking more than 112 million blogs worldwide in May of this year, a number that may include only a mere fraction of the 72 million blogs purportedly in China. 
Whatever the size, the explosive growth demands an explanation. 
Arguably, the introduction of very simple mechanisms, especially TrackBack, facilitated[15] the growth. 
If a blogger writes an entry commenting on or referring to an entry at another blog, TrackBack notifies the original blog with a "ping." 
This notification enables the original blog to display summaries of all the comments and links to them.
In this way, conversations arise spanning several blogs and rapidly form networks of individuals interested in particular themes. 
And here again large portions of the blog structure become linked via short paths --- not only the blogs and bloggers themselves but also the topics and entries made.
As blogging blossomed, researchers quickly created interesting tools, measurement techniques and data sets to try to track the dissemination of a topic through blogspace. 
Social-media analyst Matthew Hurst of Microsoft Live Labs collected link data for six weeks and produced a plot of the most active and interconnected parts of the blogosphere. 
It showed that a number of blogs are massively popular, seen by 500,000 different individuals a day. 
A link or mention of another blog by one of these superblogs guarantees a lot of traffic to the site referenced. 
The plot also shows isolated groups of dedicated enthusiasts who are very much in touch with one another but barely connect to other bloggers.
If exploited correctly, the blogosphere can be a powerful medium for spreading an idea or gauging[16] the impact of a political initiative or the likely success of a product launch. 
The much anticipated release of the Apple iPhone generated 1.4 percent of all new postings on its launch day. 
One challenge is to understand how this dissemination might change our view of journalism and commentary. 
What mechanisms can assure blog readers that the facts quoted are trustworthy? Web science can provide ways to check this so-called provenance of information, while offering practical rules about conditions surrounding its reuse. 
Daniel Weitzner's Transparent Accountable Datamining Initiative at M.I.T. is doing just that.
