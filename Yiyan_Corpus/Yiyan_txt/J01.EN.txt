CHAPTER 1THE WAVE FUNCTIONTHE SCHRODINGER EQUATIONImagine a particle of mass m, constrained to move along the x-axis, subject to some specified force F (x, t) (Figure 1.1). The program of classical mechanics is to determine the position of the particle at any given time: x (t). Once we know that, we can figure out the velocity (v = dx/dt), the momentum (p = mv), the kinetic energy (T = (1/2)mv 2 ) , or any other dynamical variable of interest. And how do we go about determining x(t)? We apply Newton's second law: F = ma.(For conservative systems - the only kind we shall consider, and, fortunately, the only kind that occur at the microscopic level - the force can be expressed as the derivative of a potential energy function, . . ., and Newton's law reads . . ..) This, together with appropriate initial conditions (typically the position and velocity at t = 0), determines x(t).Quantum mechanics approaches this same problem quite differently. In this case what we're looking for is the wave function, (x, t), of the particle, and we get it by solving the Schrodinger equation: . . ..Here i is the square root of -1, and h is Planck's constant - or rather, his original constant (h) divided by 2. . .. The Schrodinger equation plays a role logically analogous to Newton's second law: Given suitable initial conditions [typically, (x, 0)], the Schrodinger equation determines . . .(x, t) for all future time, just as, in classical mechanics, Newton's law determines x(t) for all future time.THE STATISTICAL INTERPRETATIONBut what exactly is this "wave function", and what does it do for you once you've got it? After all, a particle, by its nature, is localized at a point, whereas the wave function (as its name suggests) is spread out in space (it's a function of x, for any given time t).How can such an object be said to describe the state of a particle? The answer is provided by Born's statistical interpretation of the wave function, which says that |(. . .(x|t)|2 gives the probability of finding the particle at point x, at time t - or, more precisely, . . ..For the wave function in Figure 1.2, you would be quite likely to find the particle in the vicinity of point A, and relatively unlikely to find it near point B.The statistical interpretation introduces a kind of indeterminacy into quantum mechanics, for even if you know everything the theory has to tell you about the particle (to wit: its wave function), you cannot predict with certainty the outcome of a simple experiment to measure its position - all quantum mechanics has to offer is statistical information about the possible results.This indeterminacy has been profoundly disturbing to physicists and philosophers alike. Is it a peculiarity of nature, a deficiency in the theory, a fault in the measuring apparatus, or what? Suppose I do measure the position of the particle, and I find it to be at the point C. Question: Where was the particle just before I made the measurement?There are three plausible answers to this question, and they serve to characterize the main schools of thought regarding quantum indeterminacy: The realist position: The particle was at C. This certainly seems like a sensible response, and it is the one Einstein advocated.Note, however, that if this is true then quantum mechanics is an incomplete theory, since the particle really was at C, and yet quantum mechanics was unable to tell us so. To the realist, indeterminacy is not a fact of nature, but a reflection of our ignorance.As d'Espagnat put it, "the position of the particle was never indeterminate, but was merely unknown to the experimenter."Evidently . . . is not the whole story - some additional information (known as a hidden variable) is needed to provide a complete description of the particle.The orthodox position: The particle wasn't really anywhere.It was the act of measurement that forced the particle to "take a stand" (though how and why it decided on the point C we dare not ask). Jordan said it most starkly: "Observations not only disturb what is to be measured, they produce it. ... We compel [the particle] to assume a definite position?" This view (the so-called Copenhagen interpretation) is associated with Bohr and his followers. Among physicists it has always been the most widely accepted position.Note, however, that if it is correct there is something very peculiar about the act of measurement - something that over half a century of debate has done precious little to illuminate. The agnostic position: Refuse to answer.This is not quite as silly as it sounds - after all, what sense can there be in making assertions about the status of a particle before a measurement, when the only way of knowing whether you were right is precisely to conduct a measurement, in which case what you get is no longer "before the measurement"? It is metaphysics (in the perjorative sense of the word) to worry about something that cannot, by its nature, be tested.Pauli said, "One should no more rack one's brain about the problem of whether something one cannot know anything about exists all the same, than about the ancient question of how many angels are able to sit on the point of a needle."For decades this was the "fall-back" position of most physicists: They'd try to sell you answer 2, but if you were persistent they'd switch to 3 and terminate the conversation.Until fairly recently, all three positions (realist, orthodox, and agnostic) had their partisans.But in 1964 John Bell astonished the physics community by showing that it makes an observable difference if the particle had a precise (though unknown) position prior to the measurement.Bell's discovery effectively eliminated agnosticism as a viable option, and made it an experimental question whether 1 or 2 is the correct choice.I'll return to this story at the end of the book, when you will be in a better position to appreciate Bell's theorem; for now, suffice it to say that the experiments have confirmed decisively the orthodox interpretation: A particle simply does not have a precise position prior to measurement, any more than the ripples on a pond do; it is the measurement process that insists on one particular number, and thereby in a sense creates the specific result, limited only by the statistical weighting imposed by the wave function.But what if I made a second measurement, immediately after the first?Would I get C again, or does the act of measurement cough up some completely new number each time? On this question everyone is in agreement: A repeated measurement (on the same particle) must return the same value.Indeed, it would be tough to prove that the particle was really found at C in the first instance if this could not be confirmed by immediate repetition of the measurement.How does the orthodox interpretation account for the fact that the second measurement is bound to give the value C?Evidently the first measurement radically alters the wave function, so that it is now sharply peaked about C (Figure 1.3).We say that the wave function collapses upon measurement, to a spike at the point C (soon spreads out again, in accordance with the Schrodinger equation, so the second measurement must be made quickly).There are, then, two entirely distinct kinds of physical processes: "ordinary" ones, in which the wave function evolves in a leisurely fashion under the Schrodinger equation, and "measurements", in which . . . suddenly and discontinuously collapses.1.3 PROBABILITYBecause of the statistical interpretation, probability plays a central role in quantum mechanics, so I digress now for a brief discussion of the theory of probability.It is mainly a question of introducing some notation and terminology, and I shall do it in the context of a simple example.Imagine a room containing 14 people, whose ages are as follows: one person aged 14one person aged 15three people aged 16two people aged 22two people aged 24five people aged 25.If we let N (j) represent the number of people of age j, thenN(14) = 1N(15) = 1N(16) = 3N(22) = 2N(24) = 2N(25) = 5while N (17), for instance, is zero.The total number of people in the room is . . ..(In this instance, of course, N = 14.) Figure 1.4 is a histogram of the data.The following are some questions one might ask about this distribution.Question 1. If you selected one individual at random from this group, what is the probability that this person's age would be 15?Answer: One chance in 14, since there are 14 possible choices, all equally likely, of whom only one has this particular age.If P(j) is the probability of getting age j, then P(14) = 1/14, P(15) = 1/14, P(16) = 3/14, and so on.In general, . . ..Notice that the probability of getting either 14 or 15 is the sum of the individual probabilities (in this case, 1/7).In particular, the sum of all the probabilities is 1 - you're certain to get some age: Question 2. What is the most probable age?Answer: 25, obviously; five people share this age, whereas at most three have any other age.In general, the most probable j is the j for which P(j) is a maximum.Question 3. What is the median age?Answer: 23, for 7 people are younger than 23, and 7 are older.(In general, the median is that value of j such that the probability of getting a larger result is the same as the probability of getting a smaller result.)Question 4. What is the average (or mean) age?Answer: . . ..In general, the average value of j (which we shall write thus: (j)) is given by . . ..Notice that there need not be anyone with the average age or the median age - in this example nobody happens to be 21 or 23.In quantum mechanics the average is usually the quantity of interest; in that context it has come to be called the expectation value.It's a misleading term, since it suggests that this is the outcome you would be most likely to get if you made a single measurement (that would be the most probable value, not the average value) - but I'm afraid we're stuck with it.Question 5. What is the average of the squares of the ages?Answer: You could get 142 = 196, with probability 1/14, or 152 = 225, with probability 1/14, or 162 = 256, with probability 3/14, and so on.The average, then, is . . ..In general, the average value of some junction of j is given by . . ..(Equations 1.6, 1.7, and 1.8 are, if you like, special cases of this formula.) Beware: The average of the squares (<j2>) is not ordinarily equal to the square of the average (<j2>).For instance, if the room contains just two babies, aged 1 and 3, then <x 2> = 5, <x>2 = 4.Now, there is a conspicuous difference between the two histograms in Figure 1.5, even though they have the same median, the same average, the same most probable value, and the same number of elements: The first is sharply peaked about the average value, whereas the second is broad and flat. (The first might represent the age profile for students in a big-city classroom, and the second the pupils in a one-room schoolhouse.)We need a numerical measure of the amount of "spread" in a distribution, with respect to the average.The most obvious way to do this would be to find out how far each individual deviates from the average, . . ., and compute the average of ¦¤j.Trouble is, of course, that you get zero, since, by the nature of the average, ¦¤j is as often negative as positive: 